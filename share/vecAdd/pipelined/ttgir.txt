#blocked = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [1], order = [0], CTAsPerCGA = [1], CTASplitNum = [1], CTAOrder = [0]}>
#loc = loc("/home/hoy/triton/python/tutorials/01-vector-add.py":35:0)
#shared = #triton_gpu.shared<{vec = 4, perPhase = 1, maxPhase = 8, order = [0], CTAsPerCGA = [1], CTASplitNum = [1], CTAOrder = [0], hasLeadingOffset = true}>
module attributes {"triton_gpu.compute-capability" = 90 : i32, "triton_gpu.num-ctas" = 1 : i32, "triton_gpu.num-warps" = 1 : i32, "triton_gpu.threads-per-warp" = 32 : i32} {
  tt.func public @add_kernel_0d1d2d3de(%arg0: !tt.ptr<f32, 1> {tt.divisibility = 16 : i32} loc("/home/hoy/triton/python/tutorials/01-vector-add.py":35:0), %arg1: !tt.ptr<f32, 1> {tt.divisibility = 16 : i32} loc("/home/hoy/triton/python/tutorials/01-vector-add.py":35:0), %arg2: !tt.ptr<f32, 1> {tt.divisibility = 16 : i32} loc("/home/hoy/triton/python/tutorials/01-vector-add.py":35:0), %arg3: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 16 : i32} loc("/home/hoy/triton/python/tutorials/01-vector-add.py":35:0)) attributes {noinline = false} {
    %c3584_i32 = arith.constant 3584 : i32 loc(#loc1)
    %c-1536_i32 = arith.constant -1536 : i32 loc(#loc1)
    %c6_i32 = arith.constant 6 : i32 loc(#loc1)
    %c3072_i32 = arith.constant 3072 : i32 loc(#loc1)
    %c5_i32 = arith.constant 5 : i32 loc(#loc1)
    %c2560_i32 = arith.constant 2560 : i32 loc(#loc1)
    %cst = arith.constant dense<false> : tensor<512xi1, #blocked> loc(#loc1)
    %c4_i32 = arith.constant 4 : i32 loc(#loc1)
    %c3_i32 = arith.constant 3 : i32 loc(#loc1)
    %c1536_i32 = arith.constant 1536 : i32 loc(#loc1)
    %c2_i32 = arith.constant 2 : i32 loc(#loc1)
    %c1024_i32 = arith.constant 1024 : i32 loc(#loc1)
    %c7_i32 = arith.constant 7 : i32 loc(#loc1)
    %c1_i32 = arith.constant 1 : i32 loc(#loc1)
    %c512_i32 = arith.constant 512 : i32 loc(#loc1)
    %c0_i32 = arith.constant 0 : i32 loc(#loc1)
    %c2048_i32 = arith.constant 2048 : i32 loc(#loc1)
    %0 = tt.get_program_id x : i32 loc(#loc2)
    %1 = arith.muli %0, %c2048_i32 : i32 loc(#loc3)
    %2 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked> loc(#loc4)
    %3 = tt.splat %arg3 : (i32) -> tensor<512xi32, #blocked> loc(#loc5)
    %4 = tt.splat %arg0 : (!tt.ptr<f32, 1>) -> tensor<512x!tt.ptr<f32, 1>, #blocked> loc(#loc6)
    %5 = tt.splat %arg1 : (!tt.ptr<f32, 1>) -> tensor<512x!tt.ptr<f32, 1>, #blocked> loc(#loc7)
    %6 = tt.splat %arg2 : (!tt.ptr<f32, 1>) -> tensor<512x!tt.ptr<f32, 1>, #blocked> loc(#loc8)
    %7 = triton_gpu.alloc_tensor : tensor<7x512xf32, #shared> loc(#loc9)
    %8 = triton_gpu.alloc_tensor : tensor<7x512xf32, #shared> loc(#loc10)
    %9 = tt.splat %1 : (i32) -> tensor<512xi32, #blocked> loc(#loc11)
    %10 = arith.addi %9, %2 : tensor<512xi32, #blocked> loc(#loc11)
    %11 = arith.cmpi slt, %10, %3 : tensor<512xi32, #blocked> loc(#loc5)
    %12 = tt.addptr %4, %10 : tensor<512x!tt.ptr<f32, 1>, #blocked>, tensor<512xi32, #blocked> loc(#loc6)
    %13 = triton_gpu.insert_slice_async %12, %7, %c0_i32, %11 {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<512x!tt.ptr<f32, 1>, #blocked> -> tensor<7x512xf32, #shared> loc(#loc9)
    triton_gpu.async_commit_group loc(#loc9)
    %14 = tt.addptr %5, %10 : tensor<512x!tt.ptr<f32, 1>, #blocked>, tensor<512xi32, #blocked> loc(#loc7)
    %15 = triton_gpu.insert_slice_async %14, %8, %c0_i32, %11 {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<512x!tt.ptr<f32, 1>, #blocked> -> tensor<7x512xf32, #shared> loc(#loc10)
    triton_gpu.async_commit_group loc(#loc10)
    %16 = arith.addi %1, %c512_i32 : i32 loc(#loc12)
    %17 = tt.splat %16 : (i32) -> tensor<512xi32, #blocked> loc(#loc11)
    %18 = arith.addi %17, %2 : tensor<512xi32, #blocked> loc(#loc11)
    %19 = arith.cmpi slt, %18, %3 : tensor<512xi32, #blocked> loc(#loc5)
    %20 = tt.addptr %4, %18 : tensor<512x!tt.ptr<f32, 1>, #blocked>, tensor<512xi32, #blocked> loc(#loc6)
    %21 = triton_gpu.insert_slice_async %20, %13, %c1_i32, %19 {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<512x!tt.ptr<f32, 1>, #blocked> -> tensor<7x512xf32, #shared> loc(#loc9)
    triton_gpu.async_commit_group loc(#loc9)
    %22 = tt.addptr %5, %18 : tensor<512x!tt.ptr<f32, 1>, #blocked>, tensor<512xi32, #blocked> loc(#loc7)
    %23 = triton_gpu.insert_slice_async %22, %15, %c1_i32, %19 {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<512x!tt.ptr<f32, 1>, #blocked> -> tensor<7x512xf32, #shared> loc(#loc10)
    triton_gpu.async_commit_group loc(#loc10)
    %24 = arith.addi %1, %c1024_i32 : i32 loc(#loc12)
    %25 = tt.splat %24 : (i32) -> tensor<512xi32, #blocked> loc(#loc11)
    %26 = arith.addi %25, %2 : tensor<512xi32, #blocked> loc(#loc11)
    %27 = arith.cmpi slt, %26, %3 : tensor<512xi32, #blocked> loc(#loc5)
    %28 = tt.addptr %4, %26 : tensor<512x!tt.ptr<f32, 1>, #blocked>, tensor<512xi32, #blocked> loc(#loc6)
    %29 = triton_gpu.insert_slice_async %28, %21, %c2_i32, %27 {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<512x!tt.ptr<f32, 1>, #blocked> -> tensor<7x512xf32, #shared> loc(#loc9)
    triton_gpu.async_commit_group loc(#loc9)
    %30 = tt.addptr %5, %26 : tensor<512x!tt.ptr<f32, 1>, #blocked>, tensor<512xi32, #blocked> loc(#loc7)
    %31 = triton_gpu.insert_slice_async %30, %23, %c2_i32, %27 {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<512x!tt.ptr<f32, 1>, #blocked> -> tensor<7x512xf32, #shared> loc(#loc10)
    triton_gpu.async_commit_group loc(#loc10)
    %32 = arith.addi %1, %c1536_i32 : i32 loc(#loc12)
    %33 = tt.splat %32 : (i32) -> tensor<512xi32, #blocked> loc(#loc11)
    %34 = arith.addi %33, %2 : tensor<512xi32, #blocked> loc(#loc11)
    %35 = arith.cmpi slt, %34, %3 : tensor<512xi32, #blocked> loc(#loc5)
    %36 = tt.addptr %4, %34 : tensor<512x!tt.ptr<f32, 1>, #blocked>, tensor<512xi32, #blocked> loc(#loc6)
    %37 = triton_gpu.insert_slice_async %36, %29, %c3_i32, %35 {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<512x!tt.ptr<f32, 1>, #blocked> -> tensor<7x512xf32, #shared> loc(#loc9)
    triton_gpu.async_commit_group loc(#loc9)
    %38 = tt.addptr %5, %34 : tensor<512x!tt.ptr<f32, 1>, #blocked>, tensor<512xi32, #blocked> loc(#loc7)
    %39 = triton_gpu.insert_slice_async %38, %31, %c3_i32, %35 {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<512x!tt.ptr<f32, 1>, #blocked> -> tensor<7x512xf32, #shared> loc(#loc10)
    triton_gpu.async_commit_group loc(#loc10)
    %40 = arith.addi %1, %c2048_i32 : i32 loc(#loc12)
    %41 = tt.splat %40 : (i32) -> tensor<512xi32, #blocked> loc(#loc11)
    %42 = arith.addi %41, %2 : tensor<512xi32, #blocked> loc(#loc11)
    %43 = arith.cmpi slt, %42, %3 : tensor<512xi32, #blocked> loc(#loc5)
    %44 = tt.addptr %4, %42 : tensor<512x!tt.ptr<f32, 1>, #blocked>, tensor<512xi32, #blocked> loc(#loc6)
    %45 = triton_gpu.insert_slice_async %44, %37, %c4_i32, %cst {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<512x!tt.ptr<f32, 1>, #blocked> -> tensor<7x512xf32, #shared> loc(#loc9)
    triton_gpu.async_commit_group loc(#loc9)
    %46 = tt.addptr %5, %42 : tensor<512x!tt.ptr<f32, 1>, #blocked>, tensor<512xi32, #blocked> loc(#loc7)
    %47 = triton_gpu.insert_slice_async %46, %39, %c4_i32, %cst {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<512x!tt.ptr<f32, 1>, #blocked> -> tensor<7x512xf32, #shared> loc(#loc10)
    triton_gpu.async_commit_group loc(#loc10)
    %48 = arith.addi %1, %c2560_i32 : i32 loc(#loc12)
    %49 = tt.splat %48 : (i32) -> tensor<512xi32, #blocked> loc(#loc11)
    %50 = arith.addi %49, %2 : tensor<512xi32, #blocked> loc(#loc11)
    %51 = arith.cmpi slt, %50, %3 : tensor<512xi32, #blocked> loc(#loc5)
    %52 = tt.addptr %4, %50 : tensor<512x!tt.ptr<f32, 1>, #blocked>, tensor<512xi32, #blocked> loc(#loc6)
    %53 = triton_gpu.insert_slice_async %52, %45, %c5_i32, %cst {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<512x!tt.ptr<f32, 1>, #blocked> -> tensor<7x512xf32, #shared> loc(#loc9)
    triton_gpu.async_commit_group loc(#loc9)
    %54 = tt.addptr %5, %50 : tensor<512x!tt.ptr<f32, 1>, #blocked>, tensor<512xi32, #blocked> loc(#loc7)
    %55 = triton_gpu.insert_slice_async %54, %47, %c5_i32, %cst {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<512x!tt.ptr<f32, 1>, #blocked> -> tensor<7x512xf32, #shared> loc(#loc10)
    triton_gpu.async_commit_group loc(#loc10)
    %56 = arith.addi %1, %c3072_i32 : i32 loc(#loc12)
    %57 = tt.splat %56 : (i32) -> tensor<512xi32, #blocked> loc(#loc11)
    %58 = arith.addi %57, %2 : tensor<512xi32, #blocked> loc(#loc11)
    %59 = arith.cmpi slt, %58, %3 : tensor<512xi32, #blocked> loc(#loc5)
    %60 = tt.addptr %4, %58 : tensor<512x!tt.ptr<f32, 1>, #blocked>, tensor<512xi32, #blocked> loc(#loc6)
    %61 = triton_gpu.insert_slice_async %60, %53, %c6_i32, %cst {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<512x!tt.ptr<f32, 1>, #blocked> -> tensor<7x512xf32, #shared> loc(#loc9)
    triton_gpu.async_commit_group loc(#loc9)
    %62 = tt.addptr %5, %58 : tensor<512x!tt.ptr<f32, 1>, #blocked>, tensor<512xi32, #blocked> loc(#loc7)
    %63 = triton_gpu.insert_slice_async %62, %55, %c6_i32, %cst {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<512x!tt.ptr<f32, 1>, #blocked> -> tensor<7x512xf32, #shared> loc(#loc10)
    triton_gpu.async_commit_group loc(#loc10)
    triton_gpu.async_wait {num = 12 : i32} loc(#loc9)
    %64 = triton_gpu.extract_slice %13[%c0_i32, 0] [1, 512] [1, 1] : tensor<7x512xf32, #shared> to tensor<512xf32, #shared> loc(#loc9)
    %65 = triton_gpu.extract_slice %15[%c0_i32, 0] [1, 512] [1, 1] : tensor<7x512xf32, #shared> to tensor<512xf32, #shared> loc(#loc10)
    %66:32 = scf.for %arg4 = %c0_i32 to %c2048_i32 step %c512_i32 iter_args(%arg5 = %61, %arg6 = %63, %arg7 = %c6_i32, %arg8 = %c0_i32, %arg9 = %64, %arg10 = %65, %arg11 = %10, %arg12 = %18, %arg13 = %26, %arg14 = %34, %arg15 = %42, %arg16 = %50, %arg17 = %58, %arg18 = %11, %arg19 = %19, %arg20 = %27, %arg21 = %35, %arg22 = %43, %arg23 = %51, %arg24 = %59, %arg25 = %21, %arg26 = %29, %arg27 = %37, %arg28 = %45, %arg29 = %53, %arg30 = %61, %arg31 = %23, %arg32 = %31, %arg33 = %39, %arg34 = %47, %arg35 = %55, %arg36 = %63) -> (tensor<7x512xf32, #shared>, tensor<7x512xf32, #shared>, i32, i32, tensor<512xf32, #shared>, tensor<512xf32, #shared>, tensor<512xi32, #blocked>, tensor<512xi32, #blocked>, tensor<512xi32, #blocked>, tensor<512xi32, #blocked>, tensor<512xi32, #blocked>, tensor<512xi32, #blocked>, tensor<512xi32, #blocked>, tensor<512xi1, #blocked>, tensor<512xi1, #blocked>, tensor<512xi1, #blocked>, tensor<512xi1, #blocked>, tensor<512xi1, #blocked>, tensor<512xi1, #blocked>, tensor<512xi1, #blocked>, tensor<7x512xf32, #shared>, tensor<7x512xf32, #shared>, tensor<7x512xf32, #shared>, tensor<7x512xf32, #shared>, tensor<7x512xf32, #shared>, tensor<7x512xf32, #shared>, tensor<7x512xf32, #shared>, tensor<7x512xf32, #shared>, tensor<7x512xf32, #shared>, tensor<7x512xf32, #shared>, tensor<7x512xf32, #shared>, tensor<7x512xf32, #shared>)  : i32 {
      %67 = arith.cmpi slt, %arg4, %c-1536_i32 : i32 loc(#loc13)
      %68 = triton_gpu.convert_layout %arg9 : (tensor<512xf32, #shared>) -> tensor<512xf32, #blocked> loc(#loc9)
      %69 = triton_gpu.convert_layout %arg10 : (tensor<512xf32, #shared>) -> tensor<512xf32, #blocked> loc(#loc10)
      %70 = arith.addf %68, %69 : tensor<512xf32, #blocked> loc(#loc14)
      %71 = tt.addptr %6, %arg11 : tensor<512x!tt.ptr<f32, 1>, #blocked>, tensor<512xi32, #blocked> loc(#loc8)
      tt.store %71, %70, %arg18 {cache = 1 : i32, evict = 1 : i32} : tensor<512xf32, #blocked> loc(#loc15)
      %72 = arith.addi %arg4, %c3584_i32 : i32 loc(#loc13)
      %73 = arith.addi %1, %72 : i32 loc(#loc12)
      %74 = tt.splat %73 : (i32) -> tensor<512xi32, #blocked> loc(#loc11)
      %75 = arith.addi %74, %2 : tensor<512xi32, #blocked> loc(#loc11)
      %76 = arith.cmpi slt, %75, %3 : tensor<512xi32, #blocked> loc(#loc5)
      %77 = tt.addptr %4, %75 : tensor<512x!tt.ptr<f32, 1>, #blocked>, tensor<512xi32, #blocked> loc(#loc6)
      %78 = arith.addi %arg7, %c1_i32 : i32 loc(#loc13)
      %79 = arith.cmpi slt, %78, %c7_i32 : i32 loc(#loc13)
      %80 = arith.select %79, %78, %c0_i32 : i32 loc(#loc13)
      %81 = tt.splat %67 : (i1) -> tensor<512xi1, #blocked> loc(#loc13)
      %82 = arith.andi %81, %76 : tensor<512xi1, #blocked> loc(#loc13)
      %83 = triton_gpu.insert_slice_async %77, %arg5, %80, %82 {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<512x!tt.ptr<f32, 1>, #blocked> -> tensor<7x512xf32, #shared> loc(#loc9)
      triton_gpu.async_commit_group loc(#loc9)
      %84 = tt.addptr %5, %75 : tensor<512x!tt.ptr<f32, 1>, #blocked>, tensor<512xi32, #blocked> loc(#loc7)
      %85 = triton_gpu.insert_slice_async %84, %arg6, %80, %82 {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<512x!tt.ptr<f32, 1>, #blocked> -> tensor<7x512xf32, #shared> loc(#loc10)
      triton_gpu.async_commit_group loc(#loc10)
      %86 = arith.addi %arg8, %c1_i32 : i32 loc(#loc13)
      %87 = arith.cmpi slt, %86, %c7_i32 : i32 loc(#loc13)
      %88 = arith.select %87, %86, %c0_i32 : i32 loc(#loc13)
      triton_gpu.async_wait {num = 12 : i32} loc(#loc9)
      %89 = triton_gpu.extract_slice %arg25[%88, 0] [1, 512] [1, 1] : tensor<7x512xf32, #shared> to tensor<512xf32, #shared> loc(#loc9)
      %90 = triton_gpu.extract_slice %arg31[%88, 0] [1, 512] [1, 1] : tensor<7x512xf32, #shared> to tensor<512xf32, #shared> loc(#loc10)
      scf.yield %83, %85, %80, %88, %89, %90, %arg12, %arg13, %arg14, %arg15, %arg16, %arg17, %75, %arg19, %arg20, %arg21, %arg22, %arg23, %arg24, %76, %arg26, %arg27, %arg28, %arg29, %arg30, %83, %arg32, %arg33, %arg34, %arg35, %arg36, %85 : tensor<7x512xf32, #shared>, tensor<7x512xf32, #shared>, i32, i32, tensor<512xf32, #shared>, tensor<512xf32, #shared>, tensor<512xi32, #blocked>, tensor<512xi32, #blocked>, tensor<512xi32, #blocked>, tensor<512xi32, #blocked>, tensor<512xi32, #blocked>, tensor<512xi32, #blocked>, tensor<512xi32, #blocked>, tensor<512xi1, #blocked>, tensor<512xi1, #blocked>, tensor<512xi1, #blocked>, tensor<512xi1, #blocked>, tensor<512xi1, #blocked>, tensor<512xi1, #blocked>, tensor<512xi1, #blocked>, tensor<7x512xf32, #shared>, tensor<7x512xf32, #shared>, tensor<7x512xf32, #shared>, tensor<7x512xf32, #shared>, tensor<7x512xf32, #shared>, tensor<7x512xf32, #shared>, tensor<7x512xf32, #shared>, tensor<7x512xf32, #shared>, tensor<7x512xf32, #shared>, tensor<7x512xf32, #shared>, tensor<7x512xf32, #shared>, tensor<7x512xf32, #shared> loc(#loc13)
    } loc(#loc13)
    triton_gpu.async_wait {num = 0 : i32} loc(#loc13)
    triton_gpu.dealloc_tensor %7 : tensor<7x512xf32, #shared> loc(#loc13)
    triton_gpu.dealloc_tensor %8 : tensor<7x512xf32, #shared> loc(#loc13)
    tt.return loc(#loc16)
  } loc(#loc)
} loc(#loc)
#loc1 = loc(unknown)
#loc2 = loc("/home/hoy/triton/python/tutorials/01-vector-add.py":45:24)
#loc3 = loc("/home/hoy/triton/python/tutorials/01-vector-add.py":50:24)
#loc4 = loc("/home/hoy/triton/python/tutorials/01-vector-add.py":52:54)
#loc5 = loc("/home/hoy/triton/python/tutorials/01-vector-add.py":54:25)
#loc6 = loc("/home/hoy/triton/python/tutorials/01-vector-add.py":57:28)
#loc7 = loc("/home/hoy/triton/python/tutorials/01-vector-add.py":58:28)
#loc8 = loc("/home/hoy/triton/python/tutorials/01-vector-add.py":61:30)
#loc9 = loc("/home/hoy/triton/python/tutorials/01-vector-add.py":57:20)
#loc10 = loc("/home/hoy/triton/python/tutorials/01-vector-add.py":58:20)
#loc11 = loc("/home/hoy/triton/python/tutorials/01-vector-add.py":52:41)
#loc12 = loc("/home/hoy/triton/python/tutorials/01-vector-add.py":52:32)
#loc13 = loc("/home/hoy/triton/python/tutorials/01-vector-add.py":51:39)
#loc14 = loc("/home/hoy/triton/python/tutorials/01-vector-add.py":59:21)
#loc15 = loc("/home/hoy/triton/python/tutorials/01-vector-add.py":61:39)
#loc16 = loc("/home/hoy/triton/python/tutorials/01-vector-add.py":51:4)
